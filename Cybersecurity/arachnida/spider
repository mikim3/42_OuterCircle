#!/usr/bin/env python3
import os
import sys
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# 이미지 확장자 리스트
IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']

def is_image_url(url):
  # lower이유 확장자 대소문자 미구분하기 위함
  lower_url = url.lower()
  # 하나라도 True면 any(...) 결과 True
  return any(lower_url.endswith(ext) for ext in IMG_EXTENSIONS)

def download_image(img_url, save_path):
  try:
    # HTTP GET 요청  # stream 이를 통해 큰 파일을 효율적으로 다운로드
    r = requests.get(img_url, stream=True, timeout=10)
    # HTTP 응답 상태 확인
    r.raise_for_status()
    # with으로 파일을 열고, 블록이 끝나면 자동으로 파일을 닫음
    with open(save_path, 'wb') as f:
      for chunk in r.iter_content(chunk_size=8192):
        f.write(chunk)
    print(f"[+] Downloaded: {img_url} -> {save_path}")
    return True
  except Exception as e:
    print(f"[-] Failed to download {img_url}: {e}")
    return False

def spider_crawl(url, save_dir, depth, max_depth, visited, download_count):
  # 깊이 초과 시 중단
  if depth > max_depth:
    return download_count
  # 이미 방문한 URL이면 중단
  if url in visited:
    return download_count
  visited.add(url)

  print(f"[*] Crawling URL: {url} at depth {depth}")

  # 페이지 로드
  try:
    r = requests.get(url, timeout=10)
    r.raise_for_status()
  except requests.RequestException as e:
    print(f"[-] Failed to fetch {url}: {e}")
    return download_count

  soup = BeautifulSoup(r.text, 'html.parser')

  # 이미지 다운로드
  images = soup.find_all('img')
  if not images:
    print(f"[!] No images found at {url}")
  for img in images:
    src = img.get('src')
    if src:
      # 절대 URL로 변경
      img_url = urljoin(url, src)
      if is_image_url(img_url):
        # 파일명만 추출
        filename = os.path.basename(urlparse(img_url).path)
        # 파일명이 없을 경우 스킵
        if not filename:
          continue
        # 저장 디렉토리 생성
        if not os.path.exists(save_dir):
          os.makedirs(save_dir)
        save_path = os.path.join(save_dir, filename)
        success = download_image(img_url, save_path)
        if success:
          download_count += 1

  # 재귀로 하위 링크 추적
  links = soup.find_all('a')
  if not links:
    print(f"[!] No links found at {url}")
  for link in links:
    href = link.get('href')
    if href:
      next_url = urljoin(url, href)
      # URL 스킴 추가: 스킴이 없을 경우 'https://' 추가
      parsed_next_url = urlparse(next_url)
      if not parsed_next_url.scheme:
        next_url = 'https://' + next_url
      download_count = spider_crawl(next_url, save_dir, depth+1, max_depth, visited, download_count)

  return download_count

def main():
  args = sys.argv[1:]
  if len(args) == 0:
    print("Usage: ./spider [-r -l N -p PATH] URL")
    sys.exit(1)

  # 기본값 설정
  recursive = False
  max_depth = 5
  save_dir = './data/'
  url = None

  i = 0
  while i < len(args):
    arg = args[i]
    if arg == '-r':
      recursive = True
      i += 1
    elif arg == '-l':
      i += 1
      if i >= len(args):
        print("Error: -l option requires a number")
        sys.exit(1)
      max_depth = int(args[i])
      i += 1
    elif arg == '-p':
      i += 1
      if i >= len(args):
        print("Error: -p option requires a path")
        sys.exit(1)
      save_dir = args[i]
      i += 1
    else:
      # 남는 것은 URL
      url = arg
      i += 1

  if url is None:
    print("Usage: ./spider [-r -l N -p PATH] URL")
    sys.exit(1)

  # URL 스킴 추가: 스킴이 없을 경우 'https://' 추가
  parsed_url = urlparse(url)
  if not parsed_url.scheme:
    url = 'https://' + url

  visited = set()
  download_count = 0
  if recursive:
    download_count = spider_crawl(url, save_dir, 1, max_depth, visited, download_count)
  else:
    # 재귀 옵션이 없으면 depth=1, max_depth=1로 동일
    download_count = spider_crawl(url, save_dir, 1, 1, visited, download_count)

  print(f"[*] Download completed. Total images downloaded: {download_count}")

if __name__ == '__main__':
  main()
