#!/usr/bin/env python3
import os
import sys
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import random

# 이미지 확장자 리스트
IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']

# 무작위 User-Agent 리스트
USER_AGENTS = [
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)",
  "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)",
  "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)",
  "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko)",
  "Mozilla/5.0 (iPad; CPU OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko)"
]

# 전역 요청 수 카운트
request_count = 0
MAX_REQUESTS = 600

def is_image_url(url):
  # 확장자 처리를 위한 소문자 URL
  lower_url = url.lower()
  return any(lower_url.endswith(ext) for ext in IMG_EXTENSIONS)

def download_image(img_url, save_path):
  global request_count
  if request_count >= MAX_REQUESTS:
    print("[-] 최대 요청 수에 도달하여 더 이상 이미지를 다운로드하지 않습니다.")
    return
  headers = {
    "User-Agent": random.choice(USER_AGENTS),
    "Accept-Language": "en-US,en;q=0.9"
  }
  try:
    r = requests.get(img_url, stream=True, headers=headers, timeout=10)
    request_count += 1
    r.raise_for_status()
    with open(save_path, 'wb') as f:
      for chunk in r.iter_content(chunk_size=8192):
        f.write(chunk)
    print(f"[+] Downloaded: {img_url} -> {save_path}")
  except Exception as e:
    print(f"[-] Failed to download {img_url}: {e}")

def spider_crawl(url, save_dir, depth, max_depth, visited):
  global request_count
  if depth > max_depth:
    return
  if url in visited:
    return
  if request_count >= MAX_REQUESTS:
    print("[-] 최대 요청 수에 도달하여 크롤링을 중단합니다.")
    return
  visited.add(url)

  headers = {
    "User-Agent": random.choice(USER_AGENTS),
    "Accept-Language": "en-US,en;q=0.9"
  }

  try:
    r = requests.get(url, headers=headers, timeout=10)
    request_count += 1
    r.raise_for_status()
  except requests.RequestException as e:
    print(f"[-] Failed to fetch {url}: {e}")
    return

  try:
    soup = BeautifulSoup(r.text, 'html.parser')
  except Exception as e:
    print(f"[-] Failed to parse {url}: {e}")
    return

  # 이미지 다운로드
  images = soup.find_all('img')
  for img in images:
    src = img.get('src')
    if src:
      # URL 절대경로로 변경
      img_url = urljoin(url, src)
      if is_image_url(img_url):
        filename = os.path.basename(urlparse(img_url).path)
        if not filename:
          continue
        if not os.path.exists(save_dir):
          os.makedirs(save_dir)
        save_path = os.path.join(save_dir, filename)
        download_image(img_url, save_path)
        if request_count >= MAX_REQUESTS:
          print("[-] 최대 요청 수에 도달하여 더 이상의 이미지 다운로드를 중단합니다.")
          return

  # 재귀 탐색 옵션이 있는 경우 하위 링크 추적
  links = soup.find_all('a')
  for link in links:
    href = link.get('href')
    if href:
      next_url = urljoin(url, href)
      # http 또는 https 스킴이 아니면 이미지가 아니던가 어차피 못 다룸
      parsed_next = urlparse(next_url)
      if parsed_next.scheme not in ['http', 'https']:
        continue
      spider_crawl(next_url, save_dir, depth+1, max_depth, visited)
      if request_count >= MAX_REQUESTS:
        print("[-] 최대 요청 수에 도달하여 크롤링을 중단합니다.")
        return

def main():
  args = sys.argv[1:]
  if len(args) == 0:
    print("Usage: ./spider [-r -l N -p PATH] URL")
    sys.exit(1)

  # 기본값 설정
  recursive = False
  max_depth = 5
  save_dir = './data/'
  url = None

  i = 0
  while i < len(args):
    arg = args[i]
    if arg == '-r':
      recursive = True
      i += 1
    elif arg == '-l':
      i += 1
      if i >= len(args):
        print("Error: -l option requires a number")
        break
      try:
        max_depth = int(args[i])
        if max_depth < 1:
          print("Error: -l option requires a positive integer")
          sys.exit(1)
      except ValueError:
        print("Error: -l option requires a valid integer")
        sys.exit(1)
      i += 1
    elif arg == '-p':
      i += 1
      if i >= len(args):
        print("Error: -p option requires a path")
        sys.exit(1)
      save_dir = args[i]
      i += 1
    else:
      if url is not None:
        print(f"Error: Only allow URL")
        sys.exit(1)
      url = arg
      i += 1

  if url is None:
    print("Error: URL is required.")
    print("Usage: ./spider [-r -l N -p PATH] URL")
    sys.exit(1)

  # URL 유효성 검사
  parsed_url = urlparse(url)
  if parsed_url.scheme not in ['http', 'https']:
    print("Error: URL must start with http:// or https://")
    sys.exit(1)

  visited = set()
  if recursive:
    spider_crawl(url, save_dir, 1, max_depth, visited)
  else:
    # 재귀 옵션이 없으면 depth=1, max_depth=1로 동일
    spider_crawl(url, save_dir, 1, 1, visited)

  print(f"[+] 크롤링 완료.")

if __name__ == '__main__':
  main()
